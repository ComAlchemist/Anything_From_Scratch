{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Source: https://medium.com/data-and-beyond/complete-guide-to-building-bert-model-from-sratch-3e6562228891"
      ],
      "metadata": {
        "id": "VbgnaAW2WxB6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare Dataset\n",
        "# install packages\n",
        "!pip install transformers datasets tokenizers\n",
        "!wget http://www.cs.cornell.edu/~cristian/data/cornell_movie_dialogs_corpus.zip\n",
        "!unzip -qq cornell_movie_dialogs_corpus.zip\n",
        "!rm cornell_movie_dialogs_corpus.zip\n",
        "!mkdir datasets\n",
        "!mv cornell\\ movie-dialogs\\ corpus/movie_conversations.txt ./datasets\n",
        "!mv cornell\\ movie-dialogs\\ corpus/movie_lines.txt ./datasets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dOZpS2UxWGCl",
        "outputId": "ad5343cc-cce1-427c-e4f7-192c453dc34c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers\n",
            "  Downloading transformers-4.34.1-py3-none-any.whl (7.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.7/7.7 MB\u001b[0m \u001b[31m27.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting datasets\n",
            "  Downloading datasets-2.14.6-py3-none-any.whl (493 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m493.7/493.7 kB\u001b[0m \u001b[31m40.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tokenizers\n",
            "  Downloading tokenizers-0.14.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m77.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.4)\n",
            "Collecting huggingface-hub<1.0,>=0.16.4 (from transformers)\n",
            "  Downloading huggingface_hub-0.18.0-py3-none-any.whl (301 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.0/302.0 kB\u001b[0m \u001b[31m39.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Collecting safetensors>=0.3.1 (from transformers)\n",
            "  Downloading safetensors-0.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m72.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (9.0.0)\n",
            "Collecting dill<0.3.8,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.7-py3-none-any.whl (115 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m17.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.4.1)\n",
            "Collecting multiprocess (from datasets)\n",
            "  Downloading multiprocess-0.70.15-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m20.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec[http]<=2023.10.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.8.6)\n",
            "Collecting huggingface-hub<1.0,>=0.16.4 (from transformers)\n",
            "  Downloading huggingface_hub-0.17.3-py3-none-any.whl (295 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.0/295.0 kB\u001b[0m \u001b[31m39.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (3.3.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.5.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.3.post1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n",
            "Installing collected packages: safetensors, dill, multiprocess, huggingface-hub, tokenizers, transformers, datasets\n",
            "Successfully installed datasets-2.14.6 dill-0.3.7 huggingface-hub-0.17.3 multiprocess-0.70.15 safetensors-0.4.0 tokenizers-0.14.1 transformers-4.34.1\n",
            "--2023-10-27 09:15:47--  http://www.cs.cornell.edu/~cristian/data/cornell_movie_dialogs_corpus.zip\n",
            "Resolving www.cs.cornell.edu (www.cs.cornell.edu)... 132.236.207.36\n",
            "Connecting to www.cs.cornell.edu (www.cs.cornell.edu)|132.236.207.36|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 9916637 (9.5M) [application/zip]\n",
            "Saving to: ‘cornell_movie_dialogs_corpus.zip’\n",
            "\n",
            "cornell_movie_dialo 100%[===================>]   9.46M  11.1MB/s    in 0.9s    \n",
            "\n",
            "2023-10-27 09:15:48 (11.1 MB/s) - ‘cornell_movie_dialogs_corpus.zip’ saved [9916637/9916637]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## # import\n",
        "import os\n",
        "from pathlib import Path\n",
        "import torch\n",
        "import re\n",
        "import random\n",
        "import transformers, datasets\n",
        "from tokenizers import BertWordPieceTokenizer\n",
        "from transformers import BertTokenizer\n",
        "import tqdm\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import itertools\n",
        "import math\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "from torch.optim import Adam"
      ],
      "metadata": {
        "id": "ziaThKviW-Z_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_LEN = 64\n",
        "\n",
        "# Loading all data into memory\n",
        "corpus_movie_conv = './datasets/movie_conversations.txt'\n",
        "corpus_movie_lines = './datasets/movie_lines.txt'\n",
        "with open(corpus_movie_conv, 'r', encoding='iso-8859-1') as c:\n",
        "    conv = c.readlines()\n",
        "with open(corpus_movie_lines, 'r', encoding='iso-8859-1') as l:\n",
        "    lines = l.readlines()"
      ],
      "metadata": {
        "id": "rKy5IbUlXe_p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### splitting text using special lines\n",
        "lines_dic = {}\n",
        "for line in lines:\n",
        "    objects = line.split(\" +++$+++ \")\n",
        "    lines_dic[objects[0]] = objects[-1]"
      ],
      "metadata": {
        "id": "i4oiy0WEYRNb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### generate question answer pairs\n",
        "pairs = []\n",
        "for con in conv:\n",
        "    ids = eval(con.split(\" +++$+++ \")[-1])\n",
        "    for i in range(len(ids)):\n",
        "        qa_pairs = []\n",
        "\n",
        "        if i == len(ids) - 1:\n",
        "            break\n",
        "\n",
        "        first = lines_dic[ids[i]].strip()\n",
        "        second = lines_dic[ids[i+1]].strip()\n",
        "\n",
        "        qa_pairs.append(' '.join(first.split()[:MAX_LEN]))\n",
        "        qa_pairs.append(' '.join(second.split()[:MAX_LEN]))\n",
        "        pairs.append(qa_pairs)"
      ],
      "metadata": {
        "id": "mAB-wQdOY3qF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  WordPiece Tokenization"
      ],
      "metadata": {
        "id": "RVmZB7AabBMg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# WordPiece tokenizer\n",
        "\n",
        "### save data as txt file\n",
        "os.mkdir('./data')\n",
        "text_data = []\n",
        "file_count = 0\n",
        "\n",
        "for sample in tqdm.tqdm([x[0] for x in pairs]):\n",
        "    text_data.append(sample)\n",
        "\n",
        "    # once we hit the 10K mark, save to file\n",
        "    if len(text_data) == 10000:\n",
        "        with open(f'./data/text_{file_count}.txt', 'w', encoding='utf-8') as fp:\n",
        "            fp.write('\\n'.join(text_data))\n",
        "        text_data = []\n",
        "        file_count += 1\n",
        "\n",
        "paths = [str(x) for x in Path('./data').glob('**/*.txt')]\n",
        "\n",
        "### training own tokenizer\n",
        "tokenizer = BertWordPieceTokenizer(\n",
        "    clean_text=True,\n",
        "    handle_chinese_chars=False,\n",
        "    strip_accents=False,\n",
        "    lowercase=True\n",
        ")\n",
        "\n",
        "tokenizer.train(\n",
        "    files=paths,\n",
        "    vocab_size=30_000,\n",
        "    min_frequency=5,\n",
        "    limit_alphabet=1000,\n",
        "    wordpieces_prefix='##',\n",
        "    special_tokens=['[PAD]', '[CLS]', '[SEP]', '[MASK]', '[UNK]']\n",
        "    )\n",
        "\n",
        "os.mkdir('./bert-it-1')\n",
        "tokenizer.save_model('./bert-it-1', 'bert-it')\n",
        "tokenizer = BertTokenizer.from_pretrained('./bert-it-1/bert-it-vocab.txt', local_files_only=True)"
      ],
      "metadata": {
        "id": "yeeHDIotaKOF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "94d22bb1-6077-4446-b39c-fd7e04d5c130"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 221616/221616 [00:00<00:00, 1559733.73it/s]\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1918: FutureWarning: Calling BertTokenizer.from_pretrained() with the path to a single file or url is deprecated and won't be possible anymore in v5. Use a model identifier or the path to a directory instead.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class BERTDataset(Dataset):\n",
        "    def __init__(self, data_pair, tokenizer, seq_len=64):\n",
        "\n",
        "        self.tokenizer = tokenizer\n",
        "        self.seq_len = seq_len\n",
        "        self.corpus_lines = len(data_pair)\n",
        "        self.lines = data_pair\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.corpus_lines\n",
        "\n",
        "    def __getitem__(self, item):\n",
        "\n",
        "        # Step 1: get random sentence pair, either negative or positive (saved as is_next_label)\n",
        "        t1, t2, is_next_label = self.get_sent(item)\n",
        "\n",
        "        # Step 2: replace random words in sentence with mask / random words\n",
        "        t1_random, t1_label = self.random_word(t1)\n",
        "        t2_random, t2_label = self.random_word(t2)\n",
        "\n",
        "        # Step 3: Adding CLS and SEP tokens to the start and end of sentences\n",
        "         # Adding PAD token for labels\n",
        "        t1 = [self.tokenizer.vocab['[CLS]']] + t1_random + [self.tokenizer.vocab['[SEP]']]\n",
        "        t2 = t2_random + [self.tokenizer.vocab['[SEP]']]\n",
        "        t1_label = [self.tokenizer.vocab['[PAD]']] + t1_label + [self.tokenizer.vocab['[PAD]']]\n",
        "        t2_label = t2_label + [self.tokenizer.vocab['[PAD]']]\n",
        "\n",
        "        # Step 4: combine sentence 1 and 2 as one input\n",
        "        # adding PAD tokens to make the sentence same length as seq_len\n",
        "        segment_label = ([1 for _ in range(len(t1))] + [2 for _ in range(len(t2))])[:self.seq_len]\n",
        "        bert_input = (t1 + t2)[:self.seq_len]\n",
        "        bert_label = (t1_label + t2_label)[:self.seq_len]\n",
        "        padding = [self.tokenizer.vocab['[PAD]'] for _ in range(self.seq_len - len(bert_input))]\n",
        "        bert_input.extend(padding), bert_label.extend(padding), segment_label.extend(padding)\n",
        "\n",
        "        output = {\"bert_input\": bert_input,\n",
        "                  \"bert_label\": bert_label,\n",
        "                  \"segment_label\": segment_label,\n",
        "                  \"is_next\": is_next_label}\n",
        "\n",
        "        return {key: torch.tensor(value) for key, value in output.items()}\n",
        "\n",
        "    def random_word(self, sentence):\n",
        "        tokens = sentence.split()\n",
        "        output_label = []\n",
        "        output = []\n",
        "\n",
        "        # 15% of the tokens would be replaced\n",
        "        for i, token in enumerate(tokens):\n",
        "            prob = random.random()\n",
        "\n",
        "            # remove cls and sep token\n",
        "            token_id = self.tokenizer(token)['input_ids'][1:-1]\n",
        "\n",
        "            if prob < 0.15:\n",
        "                prob /= 0.15\n",
        "\n",
        "                # 80% chance change token to mask token\n",
        "                if prob < 0.8:\n",
        "                    for i in range(len(token_id)):\n",
        "                        output.append(self.tokenizer.vocab['[MASK]'])\n",
        "\n",
        "                # 10% chance change token to random token\n",
        "                elif prob < 0.9:\n",
        "                    for i in range(len(token_id)):\n",
        "                        output.append(random.randrange(len(self.tokenizer.vocab)))\n",
        "\n",
        "                # 10% chance change token to current token\n",
        "                else:\n",
        "                    output.append(token_id)\n",
        "\n",
        "                output_label.append(token_id)\n",
        "\n",
        "            else:\n",
        "                output.append(token_id)\n",
        "                for i in range(len(token_id)):\n",
        "                    output_label.append(0)\n",
        "\n",
        "        # flattening\n",
        "        output = list(itertools.chain(*[[x] if not isinstance(x, list) else x for x in output]))\n",
        "        output_label = list(itertools.chain(*[[x] if not isinstance(x, list) else x for x in output_label]))\n",
        "        assert len(output) == len(output_label)\n",
        "        return output, output_label\n",
        "\n",
        "    def get_sent(self, index):\n",
        "        '''return random sentence pair'''\n",
        "        t1, t2 = self.get_corpus_line(index)\n",
        "\n",
        "        # negative or positive pair, for next sentence prediction\n",
        "        if random.random() > 0.5:\n",
        "            return t1, t2, 1\n",
        "        else:\n",
        "            return t1, self.get_random_line(), 0\n",
        "\n",
        "    def get_corpus_line(self, item):\n",
        "        '''return sentence pair'''\n",
        "        return self.lines[item][0], self.lines[item][1]\n",
        "\n",
        "    def get_random_line(self):\n",
        "        '''return random single sentence'''\n",
        "        return self.lines[random.randrange(len(self.lines))][1]"
      ],
      "metadata": {
        "id": "jdjJuiTHd8BR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = BERTDataset(\n",
        "   pairs, seq_len=MAX_LEN, tokenizer=tokenizer)\n",
        "train_loader = DataLoader(\n",
        "   train_data, batch_size=32, shuffle=True, pin_memory=True)\\\n",
        "sample_data = next(iter(train_loader))\n",
        "\n",
        "print(train_data[random.randrange(len(train_data))])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 142
        },
        "id": "UrEuPbqJeaUJ",
        "outputId": "7d16c497-7d76-487d-ff75-1398fec643d7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-13-20d8ffecad0a>\"\u001b[0;36m, line \u001b[0;32m5\u001b[0m\n\u001b[0;31m    sample_data = next(iter(train_loader))\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionalEmbedding(torch.nn.Module):\n",
        "\n",
        "    def __init__(self, d_model, max_len=128):\n",
        "        super().__init__()\n",
        "\n",
        "        # Compute the positional encodings once in log space.\n",
        "        pe = torch.zeros(max_len, d_model).float()\n",
        "        pe.require_grad = False\n",
        "\n",
        "        for pos in range(max_len):\n",
        "            # for each dimension of the each position\n",
        "            for i in range(0, d_model, 2):\n",
        "                pe[pos, i] = math.sin(pos / (10000 ** ((2 * i)/d_model)))\n",
        "                pe[pos, i + 1] = math.cos(pos / (10000 ** ((2 * (i + 1))/d_model)))\n",
        "\n",
        "        # include the batch size\n",
        "        self.pe = pe.unsqueeze(0)\n",
        "        # self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.pe\n",
        "\n",
        "class BERTEmbedding(torch.nn.Module):\n",
        "    \"\"\"\n",
        "    BERT Embedding which is consisted with under features\n",
        "        1. TokenEmbedding : normal embedding matrix\n",
        "        2. PositionalEmbedding : adding positional information using sin, cos\n",
        "        2. SegmentEmbedding : adding sentence segment info, (sent_A:1, sent_B:2)\n",
        "        sum of all these features are output of BERTEmbedding\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, vocab_size, embed_size, seq_len=64, dropout=0.1):\n",
        "        \"\"\"\n",
        "        :param vocab_size: total vocab size\n",
        "        :param embed_size: embedding size of token embedding\n",
        "        :param dropout: dropout rate\n",
        "        \"\"\"\n",
        "\n",
        "        super().__init__()\n",
        "        self.embed_size = embed_size\n",
        "        # (m, seq_len) --> (m, seq_len, embed_size)\n",
        "        # padding_idx is not updated during training, remains as fixed pad (0)\n",
        "        self.token = torch.nn.Embedding(vocab_size, embed_size, padding_idx=0)\n",
        "        self.segment = torch.nn.Embedding(3, embed_size, padding_idx=0)\n",
        "        self.position = PositionalEmbedding(d_model=embed_size, max_len=seq_len)\n",
        "        self.dropout = torch.nn.Dropout(p=dropout)\n",
        "\n",
        "    def forward(self, sequence, segment_label):\n",
        "        x = self.token(sequence) + self.position(sequence) + self.segment(segment_label)\n",
        "        return self.dropout(x)"
      ],
      "metadata": {
        "id": "vRfOLZtgemHT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### attention layers\n",
        "class MultiHeadedAttention(torch.nn.Module):\n",
        "\n",
        "    def __init__(self, heads, d_model, dropout=0.1):\n",
        "        super(MultiHeadedAttention, self).__init__()\n",
        "\n",
        "        assert d_model % heads == 0\n",
        "        self.d_k = d_model // heads\n",
        "        self.heads = heads\n",
        "        self.dropout = torch.nn.Dropout(dropout)\n",
        "\n",
        "        self.query = torch.nn.Linear(d_model, d_model)\n",
        "        self.key = torch.nn.Linear(d_model, d_model)\n",
        "        self.value = torch.nn.Linear(d_model, d_model)\n",
        "        self.output_linear = torch.nn.Linear(d_model, d_model)\n",
        "\n",
        "    def forward(self, query, key, value, mask):\n",
        "        \"\"\"\n",
        "        query, key, value of shape: (batch_size, max_len, d_model)\n",
        "        mask of shape: (batch_size, 1, 1, max_words)\n",
        "        \"\"\"\n",
        "        # (batch_size, max_len, d_model)\n",
        "        query = self.query(query)\n",
        "        key = self.key(key)\n",
        "        value = self.value(value)\n",
        "\n",
        "        # (batch_size, max_len, d_model) --> (batch_size, max_len, h, d_k) --> (batch_size, h, max_len, d_k)\n",
        "        query = query.view(query.shape[0], -1, self.heads, self.d_k).permute(0, 2, 1, 3)\n",
        "        key = key.view(key.shape[0], -1, self.heads, self.d_k).permute(0, 2, 1, 3)\n",
        "        value = value.view(value.shape[0], -1, self.heads, self.d_k).permute(0, 2, 1, 3)\n",
        "\n",
        "        # (batch_size, h, max_len, d_k) matmul (batch_size, h, d_k, max_len) --> (batch_size, h, max_len, max_len)\n",
        "        scores = torch.matmul(query, key.permute(0, 1, 3, 2)) / math.sqrt(query.size(-1))\n",
        "\n",
        "        # fill 0 mask with super small number so it wont affect the softmax weight\n",
        "        # (batch_size, h, max_len, max_len)\n",
        "        scores = scores.masked_fill(mask == 0, -1e9)\n",
        "\n",
        "        # (batch_size, h, max_len, max_len)\n",
        "        # softmax to put attention weight for all non-pad tokens\n",
        "        # max_len X max_len matrix of attention\n",
        "        weights = F.softmax(scores, dim=-1)\n",
        "        weights = self.dropout(weights)\n",
        "\n",
        "        # (batch_size, h, max_len, max_len) matmul (batch_size, h, max_len, d_k) --> (batch_size, h, max_len, d_k)\n",
        "        context = torch.matmul(weights, value)\n",
        "\n",
        "        # (batch_size, h, max_len, d_k) --> (batch_size, max_len, h, d_k) --> (batch_size, max_len, d_model)\n",
        "        context = context.permute(0, 2, 1, 3).contiguous().view(context.shape[0], -1, self.heads * self.d_k)\n",
        "\n",
        "        # (batch_size, max_len, d_model)\n",
        "        return self.output_linear(context)\n",
        "\n",
        "class FeedForward(torch.nn.Module):\n",
        "    \"Implements FFN equation.\"\n",
        "\n",
        "    def __init__(self, d_model, middle_dim=2048, dropout=0.1):\n",
        "        super(FeedForward, self).__init__()\n",
        "\n",
        "        self.fc1 = torch.nn.Linear(d_model, middle_dim)\n",
        "        self.fc2 = torch.nn.Linear(middle_dim, d_model)\n",
        "        self.dropout = torch.nn.Dropout(dropout)\n",
        "        self.activation = torch.nn.GELU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.activation(self.fc1(x))\n",
        "        out = self.fc2(self.dropout(out))\n",
        "        return out\n",
        "\n",
        "class EncoderLayer(torch.nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        d_model=768,\n",
        "        heads=12,\n",
        "        feed_forward_hidden=768 * 4,\n",
        "        dropout=0.1\n",
        "        ):\n",
        "        super(EncoderLayer, self).__init__()\n",
        "        self.layernorm = torch.nn.LayerNorm(d_model)\n",
        "        self.self_multihead = MultiHeadedAttention(heads, d_model)\n",
        "        self.feed_forward = FeedForward(d_model, middle_dim=feed_forward_hidden)\n",
        "        self.dropout = torch.nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, embeddings, mask):\n",
        "        # embeddings: (batch_size, max_len, d_model)\n",
        "        # encoder mask: (batch_size, 1, 1, max_len)\n",
        "        # result: (batch_size, max_len, d_model)\n",
        "        interacted = self.dropout(self.self_multihead(embeddings, embeddings, embeddings, mask))\n",
        "        # residual layer\n",
        "        interacted = self.layernorm(interacted + embeddings)\n",
        "        # bottleneck\n",
        "        feed_forward_out = self.dropout(self.feed_forward(interacted))\n",
        "        encoded = self.layernorm(feed_forward_out + interacted)\n",
        "        return encoded"
      ],
      "metadata": {
        "id": "nGEr14Nhe7vk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BERT(torch.nn.Module):\n",
        "    \"\"\"\n",
        "    BERT model : Bidirectional Encoder Representations from Transformers.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, vocab_size, d_model=768, n_layers=12, heads=12, dropout=0.1):\n",
        "        \"\"\"\n",
        "        :param vocab_size: vocab_size of total words\n",
        "        :param hidden: BERT model hidden size\n",
        "        :param n_layers: numbers of Transformer blocks(layers)\n",
        "        :param attn_heads: number of attention heads\n",
        "        :param dropout: dropout rate\n",
        "        \"\"\"\n",
        "\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.n_layers = n_layers\n",
        "        self.heads = heads\n",
        "\n",
        "        # paper noted they used 4 * hidden_size for ff_network_hidden_size\n",
        "        self.feed_forward_hidden = d_model * 4\n",
        "\n",
        "        # embedding for BERT, sum of positional, segment, token embeddings\n",
        "        self.embedding = BERTEmbedding(vocab_size=vocab_size, embed_size=d_model)\n",
        "\n",
        "        # multi-layers transformer blocks, deep network\n",
        "        self.encoder_blocks = torch.nn.ModuleList(\n",
        "            [EncoderLayer(d_model, heads, d_model * 4, dropout) for _ in range(n_layers)])\n",
        "\n",
        "    def forward(self, x, segment_info):\n",
        "        # attention masking for padded token\n",
        "        # (batch_size, 1, seq_len, seq_len)\n",
        "        mask = (x > 0).unsqueeze(1).repeat(1, x.size(1), 1).unsqueeze(1)\n",
        "\n",
        "        # embedding the indexed sequence to sequence of vectors\n",
        "        x = self.embedding(x, segment_info)\n",
        "\n",
        "        # running over multiple transformer blocks\n",
        "        for encoder in self.encoder_blocks:\n",
        "            x = encoder.forward(x, mask)\n",
        "        return x\n",
        "\n",
        "class NextSentencePrediction(torch.nn.Module):\n",
        "    \"\"\"\n",
        "    2-class classification model : is_next, is_not_next\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, hidden):\n",
        "        \"\"\"\n",
        "        :param hidden: BERT model output size\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.linear = torch.nn.Linear(hidden, 2)\n",
        "        self.softmax = torch.nn.LogSoftmax(dim=-1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # use only the first token which is the [CLS]\n",
        "        return self.softmax(self.linear(x[:, 0]))\n",
        "\n",
        "class MaskedLanguageModel(torch.nn.Module):\n",
        "    \"\"\"\n",
        "    predicting origin token from masked input sequence\n",
        "    n-class classification problem, n-class = vocab_size\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, hidden, vocab_size):\n",
        "        \"\"\"\n",
        "        :param hidden: output size of BERT model\n",
        "        :param vocab_size: total vocab size\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.linear = torch.nn.Linear(hidden, vocab_size)\n",
        "        self.softmax = torch.nn.LogSoftmax(dim=-1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.softmax(self.linear(x))\n",
        "\n",
        "class BERTLM(torch.nn.Module):\n",
        "    \"\"\"\n",
        "    BERT Language Model\n",
        "    Next Sentence Prediction Model + Masked Language Model\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, bert: BERT, vocab_size):\n",
        "        \"\"\"\n",
        "        :param bert: BERT model which should be trained\n",
        "        :param vocab_size: total vocab size for masked_lm\n",
        "        \"\"\"\n",
        "\n",
        "        super().__init__()\n",
        "        self.bert = bert\n",
        "        self.next_sentence = NextSentencePrediction(self.bert.d_model)\n",
        "        self.mask_lm = MaskedLanguageModel(self.bert.d_model, vocab_size)\n",
        "\n",
        "    def forward(self, x, segment_label):\n",
        "        x = self.bert(x, segment_label)\n",
        "        return self.next_sentence(x), self.mask_lm(x)"
      ],
      "metadata": {
        "id": "cVH-5SlVfESW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ScheduledOptim():\n",
        "    '''A simple wrapper class for learning rate scheduling'''\n",
        "\n",
        "    def __init__(self, optimizer, d_model, n_warmup_steps):\n",
        "        self._optimizer = optimizer\n",
        "        self.n_warmup_steps = n_warmup_steps\n",
        "        self.n_current_steps = 0\n",
        "        self.init_lr = np.power(d_model, -0.5)\n",
        "\n",
        "    def step_and_update_lr(self):\n",
        "        \"Step with the inner optimizer\"\n",
        "        self._update_learning_rate()\n",
        "        self._optimizer.step()\n",
        "\n",
        "    def zero_grad(self):\n",
        "        \"Zero out the gradients by the inner optimizer\"\n",
        "        self._optimizer.zero_grad()\n",
        "\n",
        "    def _get_lr_scale(self):\n",
        "        return np.min([\n",
        "            np.power(self.n_current_steps, -0.5),\n",
        "            np.power(self.n_warmup_steps, -1.5) * self.n_current_steps])\n",
        "\n",
        "    def _update_learning_rate(self):\n",
        "        ''' Learning rate scheduling per step '''\n",
        "\n",
        "        self.n_current_steps += 1\n",
        "        lr = self.init_lr * self._get_lr_scale()\n",
        "\n",
        "        for param_group in self._optimizer.param_groups:\n",
        "            param_group['lr'] = lr"
      ],
      "metadata": {
        "id": "JyYKRMyCfGvT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BERTTrainer:\n",
        "    def __init__(\n",
        "        self,\n",
        "        model,\n",
        "        train_dataloader,\n",
        "        test_dataloader=None,\n",
        "        lr= 1e-4,\n",
        "        weight_decay=0.01,\n",
        "        betas=(0.9, 0.999),\n",
        "        warmup_steps=10000,\n",
        "        log_freq=10,\n",
        "        device='cuda'\n",
        "        ):\n",
        "\n",
        "        self.device = device\n",
        "        self.model = model\n",
        "        self.train_data = train_dataloader\n",
        "        self.test_data = test_dataloader\n",
        "\n",
        "        # Setting the Adam optimizer with hyper-param\n",
        "        self.optim = Adam(self.model.parameters(), lr=lr, betas=betas, weight_decay=weight_decay)\n",
        "        self.optim_schedule = ScheduledOptim(\n",
        "            self.optim, self.model.bert.d_model, n_warmup_steps=warmup_steps\n",
        "            )\n",
        "\n",
        "        # Using Negative Log Likelihood Loss function for predicting the masked_token\n",
        "        self.criterion = torch.nn.NLLLoss(ignore_index=0)\n",
        "        self.log_freq = log_freq\n",
        "        print(\"Total Parameters:\", sum([p.nelement() for p in self.model.parameters()]))\n",
        "\n",
        "    def train(self, epoch):\n",
        "        self.iteration(epoch, self.train_data)\n",
        "\n",
        "    def test(self, epoch):\n",
        "        self.iteration(epoch, self.test_data, train=False)\n",
        "\n",
        "    def iteration(self, epoch, data_loader, train=True):\n",
        "\n",
        "        avg_loss = 0.0\n",
        "        total_correct = 0\n",
        "        total_element = 0\n",
        "\n",
        "        mode = \"train\" if train else \"test\"\n",
        "\n",
        "        # progress bar\n",
        "        data_iter = tqdm.tqdm(\n",
        "            enumerate(data_loader),\n",
        "            desc=\"EP_%s:%d\" % (mode, epoch),\n",
        "            total=len(data_loader),\n",
        "            bar_format=\"{l_bar}{r_bar}\"\n",
        "        )\n",
        "\n",
        "        for i, data in data_iter:\n",
        "\n",
        "            # 0. batch_data will be sent into the device(GPU or cpu)\n",
        "            data = {key: value.to(self.device) for key, value in data.items()}\n",
        "\n",
        "            # 1. forward the next_sentence_prediction and masked_lm model\n",
        "            next_sent_output, mask_lm_output = self.model.forward(data[\"bert_input\"], data[\"segment_label\"])\n",
        "\n",
        "            # 2-1. NLL(negative log likelihood) loss of is_next classification result\n",
        "            next_loss = self.criterion(next_sent_output, data[\"is_next\"])\n",
        "\n",
        "            # 2-2. NLLLoss of predicting masked token word\n",
        "            # transpose to (m, vocab_size, seq_len) vs (m, seq_len)\n",
        "            # criterion(mask_lm_output.view(-1, mask_lm_output.size(-1)), data[\"bert_label\"].view(-1))\n",
        "            mask_loss = self.criterion(mask_lm_output.transpose(1, 2), data[\"bert_label\"])\n",
        "\n",
        "            # 2-3. Adding next_loss and mask_loss : 3.4 Pre-training Procedure\n",
        "            loss = next_loss + mask_loss\n",
        "\n",
        "            # 3. backward and optimization only in train\n",
        "            if train:\n",
        "                self.optim_schedule.zero_grad()\n",
        "                loss.backward()\n",
        "                self.optim_schedule.step_and_update_lr()\n",
        "\n",
        "            # next sentence prediction accuracy\n",
        "            correct = next_sent_output.argmax(dim=-1).eq(data[\"is_next\"]).sum().item()\n",
        "            avg_loss += loss.item()\n",
        "            total_correct += correct\n",
        "            total_element += data[\"is_next\"].nelement()\n",
        "\n",
        "            post_fix = {\n",
        "                \"epoch\": epoch,\n",
        "                \"iter\": i,\n",
        "                \"avg_loss\": avg_loss / (i + 1),\n",
        "                \"avg_acc\": total_correct / total_element * 100,\n",
        "                \"loss\": loss.item()\n",
        "            }\n",
        "\n",
        "            if i % self.log_freq == 0:\n",
        "                data_iter.write(str(post_fix))\n",
        "        print(\n",
        "            f\"EP{epoch}, {mode}: \\\n",
        "            avg_loss={avg_loss / len(data_iter)}, \\\n",
        "            total_acc={total_correct * 100.0 / total_element}\"\n",
        "        )"
      ],
      "metadata": {
        "id": "rMLOPzmNfIyN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''test run'''\n",
        "\n",
        "train_data = BERTDataset(\n",
        "   pairs, seq_len=MAX_LEN, tokenizer=tokenizer)\n",
        "\n",
        "train_loader = DataLoader(\n",
        "   train_data, batch_size=32, shuffle=True, pin_memory=True)\n",
        "\n",
        "bert_model = BERT(\n",
        "  vocab_size=len(tokenizer.vocab),\n",
        "  d_model=768,\n",
        "  n_layers=2,\n",
        "  heads=12,\n",
        "  dropout=0.1\n",
        ")\n",
        "\n",
        "bert_lm = BERTLM(bert_model, len(tokenizer.vocab))\n",
        "bert_trainer = BERTTrainer(bert_lm, train_loader, device='cpu')\n",
        "epochs = 20\n",
        "\n",
        "for epoch in range(epochs):\n",
        "  bert_trainer.train(epoch)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u9OIUBZifOW1",
        "outputId": "2d32e0d3-b4af-49b7-f915-a586db69e3a7"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total Parameters: 46699434\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "EP_train:0:   0%|| 1/6926 [00:11<22:31:59, 11.71s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'epoch': 0, 'iter': 0, 'avg_loss': 10.911469459533691, 'avg_acc': 56.25, 'loss': 10.911469459533691}\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "EP_train:0:   0%|| 11/6926 [01:05<10:13:09,  5.32s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'epoch': 0, 'iter': 10, 'avg_loss': 10.816637125882236, 'avg_acc': 51.98863636363637, 'loss': 10.71433162689209}\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "EP_train:0:   0%|| 21/6926 [01:57<10:03:55,  5.25s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'epoch': 0, 'iter': 20, 'avg_loss': 10.78262674240839, 'avg_acc': 51.19047619047619, 'loss': 10.68201732635498}\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "EP_train:0:   0%|| 31/6926 [02:50<10:00:56,  5.23s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'epoch': 0, 'iter': 30, 'avg_loss': 10.729352243484989, 'avg_acc': 51.20967741935484, 'loss': 10.538477897644043}\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "EP_train:0:   1%|| 41/6926 [03:43<9:56:13,  5.20s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'epoch': 0, 'iter': 40, 'avg_loss': 10.669340877998167, 'avg_acc': 52.0579268292683, 'loss': 10.434049606323242}\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "EP_train:0:   1%|| 51/6926 [04:37<10:01:37,  5.25s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'epoch': 0, 'iter': 50, 'avg_loss': 10.604647599014582, 'avg_acc': 51.28676470588235, 'loss': 10.317838668823242}\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "EP_train:0:   1%|| 61/6926 [05:31<10:35:37,  5.56s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'epoch': 0, 'iter': 60, 'avg_loss': 10.545363082260382, 'avg_acc': 51.63934426229508, 'loss': 10.150988578796387}\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "EP_train:0:   1%|| 71/6926 [06:25<9:58:43,  5.24s/it] "
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'epoch': 0, 'iter': 70, 'avg_loss': 10.490524493472677, 'avg_acc': 50.96830985915493, 'loss': 10.086946487426758}\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "EP_train:0:   1%|| 81/6926 [07:19<10:23:30,  5.47s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'epoch': 0, 'iter': 80, 'avg_loss': 10.440557833071109, 'avg_acc': 51.23456790123457, 'loss': 10.0697021484375}\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "EP_train:0:   1%|| 91/6926 [08:12<9:54:27,  5.22s/it] "
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'epoch': 0, 'iter': 90, 'avg_loss': 10.390255225883735, 'avg_acc': 50.755494505494504, 'loss': 9.871966361999512}\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "EP_train:0:   1%|| 101/6926 [09:06<10:12:57,  5.39s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'epoch': 0, 'iter': 100, 'avg_loss': 10.34093991836699, 'avg_acc': 50.99009900990099, 'loss': 9.806042671203613}\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "EP_train:0:   2%|| 111/6926 [10:03<11:24:36,  6.03s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'epoch': 0, 'iter': 110, 'avg_loss': 10.292244060619458, 'avg_acc': 50.929054054054056, 'loss': 9.830770492553711}\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "EP_train:0:   2%|| 121/6926 [10:56<9:59:46,  5.29s/it] "
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'epoch': 0, 'iter': 120, 'avg_loss': 10.237472077046544, 'avg_acc': 50.981404958677686, 'loss': 9.56406307220459}\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "EP_train:0:   2%|| 131/6926 [11:51<10:32:22,  5.58s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'epoch': 0, 'iter': 130, 'avg_loss': 10.182017319075024, 'avg_acc': 51.14503816793893, 'loss': 9.40656566619873}\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "EP_train:0:   2%|| 141/6926 [12:44<9:56:47,  5.28s/it] "
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'epoch': 0, 'iter': 140, 'avg_loss': 10.12651527350676, 'avg_acc': 51.10815602836879, 'loss': 9.193609237670898}\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "EP_train:0:   2%|| 151/6926 [13:38<10:12:33,  5.42s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'epoch': 0, 'iter': 150, 'avg_loss': 10.065764787181324, 'avg_acc': 50.95198675496688, 'loss': 9.024138450622559}\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "EP_train:0:   2%|| 161/6926 [14:31<9:40:18,  5.15s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'epoch': 0, 'iter': 160, 'avg_loss': 10.004589637614185, 'avg_acc': 51.08695652173913, 'loss': 9.243189811706543}\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "EP_train:0:   2%|| 171/6926 [15:25<10:11:38,  5.43s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'epoch': 0, 'iter': 170, 'avg_loss': 9.944770857604624, 'avg_acc': 51.27923976608187, 'loss': 9.09669303894043}\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "EP_train:0:   3%|| 181/6926 [16:20<10:10:26,  5.43s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'epoch': 0, 'iter': 180, 'avg_loss': 9.88215657228923, 'avg_acc': 51.07044198895028, 'loss': 8.827354431152344}\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "EP_train:0:   3%|| 191/6926 [17:14<10:10:16,  5.44s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'epoch': 0, 'iter': 190, 'avg_loss': 9.819761181376991, 'avg_acc': 51.014397905759154, 'loss': 8.287186622619629}\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "EP_train:0:   3%|| 201/6926 [18:07<9:49:31,  5.26s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'epoch': 0, 'iter': 200, 'avg_loss': 9.760896744419686, 'avg_acc': 50.82400497512438, 'loss': 8.625864028930664}\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "EP_train:0:   3%|| 211/6926 [19:01<9:57:36,  5.34s/it] "
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'epoch': 0, 'iter': 210, 'avg_loss': 9.695868469527547, 'avg_acc': 50.622037914691944, 'loss': 8.24477481842041}\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "EP_train:0:   3%|| 221/6926 [19:54<9:50:10,  5.28s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'epoch': 0, 'iter': 220, 'avg_loss': 9.638503048754385, 'avg_acc': 50.77771493212669, 'loss': 8.20501708984375}\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "EP_train:0:   3%|| 231/6926 [20:48<9:51:23,  5.30s/it] "
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'epoch': 0, 'iter': 230, 'avg_loss': 9.583613032386417, 'avg_acc': 50.879329004329, 'loss': 8.420318603515625}\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "EP_train:0:   3%|| 241/6926 [21:40<9:55:13,  5.34s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'epoch': 0, 'iter': 240, 'avg_loss': 9.525782866102036, 'avg_acc': 50.71317427385892, 'loss': 8.227885246276855}\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "EP_train:0:   4%|| 251/6926 [22:36<10:03:44,  5.43s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'epoch': 0, 'iter': 250, 'avg_loss': 9.464566794999568, 'avg_acc': 50.74701195219124, 'loss': 7.548211097717285}\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "EP_train:0:   4%|| 261/6926 [23:30<10:20:46,  5.59s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'epoch': 0, 'iter': 260, 'avg_loss': 9.410918275971978, 'avg_acc': 50.92193486590039, 'loss': 7.743703365325928}\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "EP_train:0:   4%|| 271/6926 [24:26<10:03:08,  5.44s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'epoch': 0, 'iter': 270, 'avg_loss': 9.357718638388434, 'avg_acc': 50.89944649446494, 'loss': 7.604986190795898}\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "EP_train:0:   4%|| 281/6926 [25:21<10:06:00,  5.47s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'epoch': 0, 'iter': 280, 'avg_loss': 9.311280510179513, 'avg_acc': 51.06761565836299, 'loss': 7.713982582092285}\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "EP_train:0:   4%|| 291/6926 [26:15<9:58:46,  5.41s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'epoch': 0, 'iter': 290, 'avg_loss': 9.266376523217794, 'avg_acc': 50.998711340206185, 'loss': 8.136502265930176}\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "EP_train:0:   4%|| 301/6926 [27:10<9:56:25,  5.40s/it] "
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'epoch': 0, 'iter': 300, 'avg_loss': 9.221592515013938, 'avg_acc': 51.0797342192691, 'loss': 7.937389373779297}\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "EP_train:0:   4%|| 311/6926 [28:05<10:28:37,  5.70s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'epoch': 0, 'iter': 310, 'avg_loss': 9.173889127958242, 'avg_acc': 50.86414790996785, 'loss': 7.576677322387695}\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "EP_train:0:   5%|| 321/6926 [29:00<9:37:44,  5.25s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'epoch': 0, 'iter': 320, 'avg_loss': 9.127501514470465, 'avg_acc': 50.866433021806856, 'loss': 7.559384346008301}\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "EP_train:0:   5%|| 331/6926 [29:57<10:58:35,  5.99s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'epoch': 0, 'iter': 330, 'avg_loss': 9.080808919001923, 'avg_acc': 50.698640483383684, 'loss': 7.584811210632324}\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "EP_train:0:   5%|| 341/6926 [30:52<9:52:44,  5.40s/it] "
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'epoch': 0, 'iter': 340, 'avg_loss': 9.037170626900412, 'avg_acc': 50.78812316715543, 'loss': 7.140135288238525}\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "EP_train:0:   5%|| 351/6926 [31:47<9:52:41,  5.41s/it] "
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'epoch': 0, 'iter': 350, 'avg_loss': 8.994889495719192, 'avg_acc': 50.79237891737892, 'loss': 7.248659610748291}\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "EP_train:0:   5%|| 361/6926 [32:43<10:15:18,  5.62s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'epoch': 0, 'iter': 360, 'avg_loss': 8.957462071711998, 'avg_acc': 50.82236842105263, 'loss': 7.508889198303223}\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "EP_train:0:   5%|| 371/6926 [33:34<9:20:55,  5.13s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'epoch': 0, 'iter': 370, 'avg_loss': 8.916847146746283, 'avg_acc': 50.81704851752021, 'loss': 7.579354286193848}\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "EP_train:0:   6%|| 381/6926 [34:27<9:48:06,  5.39s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'epoch': 0, 'iter': 380, 'avg_loss': 8.878703803215126, 'avg_acc': 50.9022309711286, 'loss': 7.514769077301025}\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "EP_train:0:   6%|| 391/6926 [35:20<9:27:44,  5.21s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'epoch': 0, 'iter': 390, 'avg_loss': 8.839476053672069, 'avg_acc': 50.97506393861892, 'loss': 7.144907474517822}\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "EP_train:0:   6%|| 401/6926 [36:15<9:58:08,  5.50s/it] "
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'epoch': 0, 'iter': 400, 'avg_loss': 8.803268063989957, 'avg_acc': 50.935162094763086, 'loss': 7.673941135406494}\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "EP_train:0:   6%|| 411/6926 [37:08<9:32:25,  5.27s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'epoch': 0, 'iter': 410, 'avg_loss': 8.767126598497377, 'avg_acc': 50.88199513381995, 'loss': 7.32290506362915}\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "EP_train:0:   6%|| 421/6926 [38:01<9:26:46,  5.23s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'epoch': 0, 'iter': 420, 'avg_loss': 8.727823164570642, 'avg_acc': 50.93527315914489, 'loss': 6.71901273727417}\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "EP_train:0:   6%|| 431/6926 [38:54<9:28:49,  5.25s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'epoch': 0, 'iter': 430, 'avg_loss': 8.690767454160616, 'avg_acc': 51.09483758700696, 'loss': 7.224283218383789}\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "EP_train:0:   6%|| 441/6926 [39:46<9:23:57,  5.22s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'epoch': 0, 'iter': 440, 'avg_loss': 8.655209274248742, 'avg_acc': 51.16213151927438, 'loss': 7.236321449279785}\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "EP_train:0:   7%|| 451/6926 [40:39<9:41:05,  5.38s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'epoch': 0, 'iter': 450, 'avg_loss': 8.621393580130093, 'avg_acc': 51.15022172949002, 'loss': 7.106378078460693}\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "EP_train:0:   7%|| 461/6926 [41:32<9:23:42,  5.23s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'epoch': 0, 'iter': 460, 'avg_loss': 8.58861198963157, 'avg_acc': 51.05070498915401, 'loss': 7.180912494659424}\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "EP_train:0:   7%|| 471/6926 [42:25<9:40:47,  5.40s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'epoch': 0, 'iter': 470, 'avg_loss': 8.554730396108768, 'avg_acc': 51.08147558386412, 'loss': 7.176949977874756}\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "EP_train:0:   7%|| 481/6926 [43:16<9:14:07,  5.16s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'epoch': 0, 'iter': 480, 'avg_loss': 8.524636401456013, 'avg_acc': 51.01351351351351, 'loss': 7.1140007972717285}\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "EP_train:0:   7%|| 491/6926 [44:09<9:31:14,  5.33s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'epoch': 0, 'iter': 490, 'avg_loss': 8.493296294979555, 'avg_acc': 51.01196537678207, 'loss': 7.628721714019775}\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "EP_train:0:   7%|| 501/6926 [45:01<9:18:41,  5.22s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'epoch': 0, 'iter': 500, 'avg_loss': 8.459842394449991, 'avg_acc': 51.03542914171657, 'loss': 7.131675720214844}\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "EP_train:0:   7%|| 511/6926 [45:54<9:34:58,  5.38s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'epoch': 0, 'iter': 510, 'avg_loss': 8.429969276234129, 'avg_acc': 50.978473581213315, 'loss': 7.116626262664795}\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "EP_train:0:   8%|| 521/6926 [46:47<9:14:35,  5.20s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'epoch': 0, 'iter': 520, 'avg_loss': 8.39781265844539, 'avg_acc': 50.8697216890595, 'loss': 6.542564868927002}\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "EP_train:0:   8%|| 531/6926 [47:42<10:12:41,  5.75s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'epoch': 0, 'iter': 530, 'avg_loss': 8.366487942172983, 'avg_acc': 50.80037664783428, 'loss': 6.740990161895752}\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "EP_train:0:   8%|| 541/6926 [48:34<9:17:48,  5.24s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'epoch': 0, 'iter': 540, 'avg_loss': 8.33860232252731, 'avg_acc': 50.947319778188536, 'loss': 6.855205535888672}\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "EP_train:0:   8%|| 551/6926 [49:27<9:42:31,  5.48s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'epoch': 0, 'iter': 550, 'avg_loss': 8.309950023727279, 'avg_acc': 50.9641560798548, 'loss': 6.8635969161987305}\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "EP_train:0:   8%|| 561/6926 [50:19<9:04:06,  5.13s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'epoch': 0, 'iter': 560, 'avg_loss': 8.2812789961191, 'avg_acc': 50.991532976827095, 'loss': 6.333866596221924}\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "EP_train:0:   8%|| 571/6926 [51:12<9:39:09,  5.47s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'epoch': 0, 'iter': 570, 'avg_loss': 8.25473693206306, 'avg_acc': 50.97964098073555, 'loss': 6.908053874969482}\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "EP_train:0:   8%|| 581/6926 [52:07<10:08:03,  5.75s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'epoch': 0, 'iter': 580, 'avg_loss': 8.228953288467329, 'avg_acc': 51.00043029259896, 'loss': 6.338192462921143}\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "EP_train:0:   9%|| 591/6926 [53:03<9:49:36,  5.58s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'epoch': 0, 'iter': 590, 'avg_loss': 8.201602334701874, 'avg_acc': 51.02580372250423, 'loss': 6.377570629119873}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "EP_train:0:   9%|| 601/6926 [53:59<9:43:16,  5.53s/it] "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'epoch': 0, 'iter': 600, 'avg_loss': 8.176689946909315, 'avg_acc': 50.972337770382694, 'loss': 7.002769470214844}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "EP_train:0:   9%|| 611/6926 [54:55<9:40:15,  5.51s/it] "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'epoch': 0, 'iter': 610, 'avg_loss': 8.150368168389154, 'avg_acc': 50.99222585924713, 'loss': 6.239799976348877}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "EP_train:0:   9%|| 620/6926 [55:44<9:25:07,  5.38s/it]"
          ]
        }
      ]
    }
  ]
}