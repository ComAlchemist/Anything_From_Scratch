{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":5572840,"sourceType":"datasetVersion","datasetId":3207741}],"dockerImageVersionId":30498,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install -q -U bitsandbytes\n!pip install -q -U git+https://github.com/huggingface/transformers.git \n!pip install -q -U git+https://github.com/huggingface/peft.git\n!pip install -q -U git+https://github.com/huggingface/accelerate.git\n!pip install -q datasets","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_name = \"EleutherAI/gpt-neox-20b\"\n\n#Tokenizer\ntokenizer = AutoTokenizer.from_pretrained(model_name)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"quant_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_use_double_quant=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=torch.bfloat16\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = AutoModelForCausalLM.from_pretrained(model_name, quantization_config=quant_config, device_map={\"\":0})","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.gradient_checkpointing_enable()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from peft import prepare_model_for_kbit_training, LoraConfig, get_peft_model\n\nmodel = prepare_model_for_kbit_training(model)\n\nconfig = LoraConfig(\n    r=8, \n    lora_alpha=32, \n    target_modules=[\"query_key_value\"], \n    lora_dropout=0.05, \n    bias=\"none\", \n    task_type=\"CAUSAL_LM\"\n)\n\nmodel = get_peft_model(model, config)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from datasets import load_dataset\ndata = load_dataset(\"Abirate/english_quotes\")\ndata = data.map(lambda samples: tokenizer(samples[\"quote\"]), batched=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import transformers\n\ntokenizer.pad_token = tokenizer.eos_token\n\ntrainer = transformers.Trainer(\n    model=model,\n    train_dataset=data[\"train\"],\n    args=transformers.TrainingArguments(\n        per_device_train_batch_size=1,\n        gradient_accumulation_steps=8,\n        warmup_steps=2,\n        max_steps=20,\n        learning_rate=2e-4,\n        fp16=True,\n        logging_steps=1,\n        output_dir=\"outputs\",\n        optim=\"paged_adamw_8bit\"\n    ),\n    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False),\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainer.train()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"text = \"How to learn python?\"\ndevice = \"cuda:0\"\ninputs = tokenizer(text, return_tensors=\"pt\").to(device)\n\noutputs = model.generate(**inputs, max_new_tokens=20)\nprint(tokenizer.decode(outputs[0], skip_special_tokens=True))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}