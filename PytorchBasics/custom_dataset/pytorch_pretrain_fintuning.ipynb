{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3MCdfZjzWfXR"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Shows a small example of how to load a pretrain model (VGG16) from PyTorch,\n",
        "and modifies this to train on the CIFAR10 dataset. The same method generalizes\n",
        "well to other datasets, but the modifications to the network may need to be changed.\n",
        "\n",
        "Programmed by Aladdin Persson <aladdin.persson at hotmail dot com>\n",
        "*    2020-04-08 Initial coding\n",
        "*    2022-12-19 Updated comments, minor code changes, made sure it works with latest PyTorch\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "# Imports\n",
        "import torch\n",
        "import torchvision\n",
        "import torch.nn as nn  # All neural network modules, nn.Linear, nn.Conv2d, BatchNorm, Loss functions\n",
        "import torch.optim as optim  # For all Optimization algorithms, SGD, Adam, etc.\n",
        "import torch.nn.functional as F  # All functions that don't have any parameters\n",
        "from torch.utils.data import (\n",
        "    DataLoader,\n",
        ")  # Gives easier dataset managment and creates mini batches\n",
        "import torchvision.datasets as datasets  # Has standard datasets we can import in a nice way\n",
        "import torchvision.transforms as transforms  # Transformations we can perform on our dataset\n",
        "from tqdm import tqdm\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Hyperparameters\n",
        "num_classes = 10\n",
        "learning_rate = 1e-3\n",
        "batch_size = 1024\n",
        "num_epochs = 5\n",
        "\n",
        "# Load pretrain model & modify it\n",
        "model = torchvision.models.vgg16(weights=\"DEFAULT\")\n",
        "\n",
        "# If you want to do finetuning then set requires_grad = False\n",
        "# Remove these two lines if you want to train entire model,\n",
        "# and only want to load the pretrain weights.\n",
        "for param in model.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "model.avgpool = nn.Identity()\n",
        "model.classifier = nn.Sequential(\n",
        "    nn.Linear(512, 100), nn.ReLU(), nn.Linear(100, num_classes)\n",
        ")\n",
        "model.to(device)\n",
        "\n",
        "\n",
        "# Load Data\n",
        "train_dataset = datasets.CIFAR10(\n",
        "    root=\"dataset/\", train=True, transform=transforms.ToTensor(), download=True\n",
        ")\n",
        "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "# Loss and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Train Network\n",
        "for epoch in range(num_epochs):\n",
        "    losses = []\n",
        "\n",
        "    for batch_idx, (data, targets) in enumerate(tqdm(train_loader)):\n",
        "        # Get data to cuda if possible\n",
        "        data = data.to(device=device)\n",
        "        targets = targets.to(device=device)\n",
        "\n",
        "        # forward\n",
        "        scores = model(data)\n",
        "        loss = criterion(scores, targets)\n",
        "\n",
        "        losses.append(loss.item())\n",
        "        # backward\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "\n",
        "        # gradient descent or adam step\n",
        "        optimizer.step()\n",
        "\n",
        "    print(f\"Cost at epoch {epoch} is {sum(losses)/len(losses):.5f}\")\n",
        "\n",
        "# Check accuracy on training & test to see how good our model\n",
        "\n",
        "\n",
        "def check_accuracy(loader, model):\n",
        "    if loader.dataset.train:\n",
        "        print(\"Checking accuracy on training data\")\n",
        "    else:\n",
        "        print(\"Checking accuracy on test data\")\n",
        "\n",
        "    num_correct = 0\n",
        "    num_samples = 0\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for x, y in loader:\n",
        "            x = x.to(device=device)\n",
        "            y = y.to(device=device)\n",
        "\n",
        "            scores = model(x)\n",
        "            _, predictions = scores.max(1)\n",
        "            num_correct += (predictions == y).sum()\n",
        "            num_samples += predictions.size(0)\n",
        "\n",
        "        print(\n",
        "            f\"Got {num_correct} / {num_samples} with accuracy {float(num_correct)/float(num_samples)*100:.2f}\"\n",
        "        )\n",
        "\n",
        "    model.train()\n",
        "\n",
        "\n",
        "check_accuracy(train_loader, model)"
      ]
    }
  ]
}