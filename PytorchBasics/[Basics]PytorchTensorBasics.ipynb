{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7hTjX7wKzk6G",
        "outputId": "fd13ae21-70e8-4b0d-c9cc-210a685dbcb4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Information about tensor: tensor([[1., 2., 3.],\n",
            "        [4., 5., 6.]], requires_grad=True)\n",
            "Type of Tensor {my_tensor.dtype}\n",
            "Device Tensor is on cpu\n",
            "Shape of tensor torch.Size([2, 3])\n",
            "Requires gradient: True\n",
            "Converted Boolean: tensor([False,  True,  True,  True])\n",
            "Converted int16 tensor([0, 1, 2, 3], dtype=torch.int16)\n",
            "Converted int64 tensor([0, 1, 2, 3])\n",
            "Converted float16 tensor([0., 1., 2., 3.], dtype=torch.float16)\n",
            "Converted float32 tensor([0., 1., 2., 3.])\n",
            "Converted float64 tensor([0., 1., 2., 3.], dtype=torch.float64)\n",
            "tensor([[6.6746, 4.2502, 4.4485, 6.2116, 4.9541],\n",
            "        [4.6610, 2.9231, 3.2854, 4.8047, 3.5150],\n",
            "        [4.0275, 2.6091, 2.5246, 3.4874, 2.8813],\n",
            "        [6.2351, 3.9031, 4.3030, 6.0992, 4.6584],\n",
            "        [4.0241, 2.5950, 2.4865, 3.7738, 2.8336]])\n",
            "torch.Size([25])\n",
            "torch.Size([10])\n",
            "torch.Size([10])\n",
            "tensor([2, 5, 8])\n",
            "tensor([0.6130, 0.2337])\n",
            "tensor([0, 1, 9])\n",
            "tensor([0, 2, 4, 6, 8])\n",
            "tensor([ 0,  2,  4,  6,  8, 10,  6,  7,  8,  9])\n",
            "1\n",
            "10\n",
            "False\n",
            "tensor([0, 3, 6, 1, 4, 7, 2, 5, 8])\n",
            "torch.Size([4, 5])\n",
            "torch.Size([2, 10])\n",
            "torch.Size([64, 1, 5])\n",
            "torch.Size([64, 1, 5])\n",
            "torch.Size([1, 10])\n",
            "torch.Size([10, 1])\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "Walk through of a lot of different useful Tensor Operations, where we\n",
        "go through what I think are four main parts in:\n",
        "\n",
        "1. Initialization of a Tensor\n",
        "2. Tensor Mathematical Operations and Comparison\n",
        "3. Tensor Indexing\n",
        "4. Tensor Reshaping\n",
        "\n",
        "But also other things such as setting the device (GPU/CPU) and converting\n",
        "between different types (int, float etc) and how to convert a tensor to an\n",
        "numpy array and vice-versa.\n",
        "\n",
        "Programmed by Aladdin Persson\n",
        "* 2020-06-27: Initial coding\n",
        "* 2022-12-19: Small revision of code, checked that it works with latest PyTorch version\n",
        "\"\"\"\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "# ================================================================= #\n",
        "#                        Initializing Tensor                        #\n",
        "# ================================================================= #\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"  # Cuda to run on GPU!\n",
        "\n",
        "# Initializing a Tensor in this case of shape 2x3 (2 rows, 3 columns)\n",
        "my_tensor = torch.tensor(\n",
        "    [[1, 2, 3], [4, 5, 6]], dtype=torch.float32, device=device, requires_grad=True\n",
        ")\n",
        "\n",
        "# A few tensor attributes\n",
        "print(\n",
        "    f\"Information about tensor: {my_tensor}\"\n",
        ")  # Prints data of the tensor, device and grad info\n",
        "print(\n",
        "    \"Type of Tensor {my_tensor.dtype}\"\n",
        ")  # Prints dtype of the tensor (torch.float32, etc)\n",
        "print(\n",
        "    f\"Device Tensor is on {my_tensor.device}\"\n",
        ")  # Prints cpu/cuda (followed by gpu number)\n",
        "print(f\"Shape of tensor {my_tensor.shape}\")  # Prints shape, in this case 2x3\n",
        "print(f\"Requires gradient: {my_tensor.requires_grad}\")  # Prints true/false\n",
        "\n",
        "# Other common initialization methods (there exists a ton more)\n",
        "x = torch.empty(size=(3, 3))  # Tensor of shape 3x3 with uninitialized data\n",
        "x = torch.zeros((3, 3))  # Tensor of shape 3x3 with values of 0\n",
        "x = torch.rand(\n",
        "    (3, 3)\n",
        ")  # Tensor of shape 3x3 with values from uniform distribution in interval [0,1)\n",
        "x = torch.ones((3, 3))  # Tensor of shape 3x3 with values of 1\n",
        "x = torch.eye(5, 5)  # Returns Identity Matrix I, (I <-> Eye), matrix of shape 2x3\n",
        "x = torch.arange(\n",
        "    start=0, end=5, step=1\n",
        ")  # Tensor [0, 1, 2, 3, 4], note, can also do: torch.arange(11)\n",
        "x = torch.linspace(start=0.1, end=1, steps=10)  # x = [0.1, 0.2, ..., 1]\n",
        "x = torch.empty(size=(1, 5)).normal_(\n",
        "    mean=0, std=1\n",
        ")  # Normally distributed with mean=0, std=1\n",
        "x = torch.empty(size=(1, 5)).uniform_(\n",
        "    0, 1\n",
        ")  # Values from a uniform distribution low=0, high=1\n",
        "x = torch.diag(torch.ones(3))  # Diagonal matrix of shape 3x3\n",
        "\n",
        "# How to make initialized tensors to other types (int, float, double)\n",
        "# These will work even if you're on CPU or CUDA!\n",
        "tensor = torch.arange(4)  # [0, 1, 2, 3] Initialized as int64 by default\n",
        "print(f\"Converted Boolean: {tensor.bool()}\")  # Converted to Boolean: 1 if nonzero\n",
        "print(f\"Converted int16 {tensor.short()}\")  # Converted to int16\n",
        "print(\n",
        "    f\"Converted int64 {tensor.long()}\"\n",
        ")  # Converted to int64 (This one is very important, used super often)\n",
        "print(f\"Converted float16 {tensor.half()}\")  # Converted to float16\n",
        "print(\n",
        "    f\"Converted float32 {tensor.float()}\"\n",
        ")  # Converted to float32 (This one is very important, used super often)\n",
        "print(f\"Converted float64 {tensor.double()}\")  # Converted to float64\n",
        "\n",
        "# Array to Tensor conversion and vice-versa\n",
        "np_array = np.zeros((5, 5))\n",
        "tensor = torch.from_numpy(np_array)\n",
        "np_array_again = (\n",
        "    tensor.numpy()\n",
        ")  # np_array_again will be same as np_array (perhaps with numerical round offs)\n",
        "\n",
        "# =============================================================================== #\n",
        "#                        Tensor Math & Comparison Operations                      #\n",
        "# =============================================================================== #\n",
        "\n",
        "x = torch.tensor([1, 2, 3])\n",
        "y = torch.tensor([9, 8, 7])\n",
        "\n",
        "# -- Addition --\n",
        "z1 = torch.empty(3)\n",
        "torch.add(x, y, out=z1)  # This is one way\n",
        "z2 = torch.add(x, y)  # This is another way\n",
        "z = x + y  # This is my preferred way, simple and clean.\n",
        "\n",
        "# -- Subtraction --\n",
        "z = x - y  # We can do similarly as the preferred way of addition\n",
        "\n",
        "# -- Division (A bit clunky) --\n",
        "z = torch.true_divide(x, y)  # Will do element wise division if of equal shape\n",
        "\n",
        "# -- Inplace Operations --\n",
        "t = torch.zeros(3)\n",
        "\n",
        "t.add_(x)  # Whenever we have operation followed by _ it will mutate the tensor in place\n",
        "t += x  # Also inplace: t = t + x is not inplace, bit confusing.\n",
        "\n",
        "# -- Exponentiation (Element wise if vector or matrices) --\n",
        "z = x.pow(2)  # z = [1, 4, 9]\n",
        "z = x**2  # z = [1, 4, 9]\n",
        "\n",
        "\n",
        "# -- Simple Comparison --\n",
        "z = x > 0  # Returns [True, True, True]\n",
        "z = x < 0  # Returns [False, False, False]\n",
        "\n",
        "# -- Matrix Multiplication --\n",
        "x1 = torch.rand((2, 5))\n",
        "x2 = torch.rand((5, 3))\n",
        "x3 = torch.mm(x1, x2)  # Matrix multiplication of x1 and x2, out shape: 2x3\n",
        "x3 = x1.mm(x2)  # Similar as line above\n",
        "\n",
        "# -- Matrix Exponentiation --\n",
        "matrix_exp = torch.rand(5, 5)\n",
        "print(\n",
        "    matrix_exp.matrix_power(3)\n",
        ")  # is same as matrix_exp (mm) matrix_exp (mm) matrix_exp\n",
        "\n",
        "# -- Element wise Multiplication --\n",
        "z = x * y  # z = [9, 16, 21] = [1*9, 2*8, 3*7]\n",
        "\n",
        "# -- Dot product --\n",
        "z = torch.dot(x, y)  # Dot product, in this case z = 1*9 + 2*8 + 3*7\n",
        "\n",
        "# -- Batch Matrix Multiplication --\n",
        "batch = 32\n",
        "n = 10\n",
        "m = 20\n",
        "p = 30\n",
        "tensor1 = torch.rand((batch, n, m))\n",
        "tensor2 = torch.rand((batch, m, p))\n",
        "out_bmm = torch.bmm(tensor1, tensor2)  # Will be shape: (b x n x p)\n",
        "\n",
        "# -- Example of broadcasting --\n",
        "x1 = torch.rand((5, 5))\n",
        "x2 = torch.ones((1, 5))\n",
        "z = (\n",
        "    x1 - x2\n",
        ")  # Shape of z is 5x5: How? The 1x5 vector (x2) is subtracted for each row in the 5x5 (x1)\n",
        "z = (\n",
        "    x1**x2\n",
        ")  # Shape of z is 5x5: How? Broadcasting! Element wise exponentiation for every row\n",
        "\n",
        "# Other useful tensor operations\n",
        "sum_x = torch.sum(\n",
        "    x, dim=0\n",
        ")  # Sum of x across dim=0 (which is the only dim in our case), sum_x = 6\n",
        "values, indices = torch.max(x, dim=0)  # Can also do x.max(dim=0)\n",
        "values, indices = torch.min(x, dim=0)  # Can also do x.min(dim=0)\n",
        "abs_x = torch.abs(x)  # Returns x where abs function has been applied to every element\n",
        "z = torch.argmax(x, dim=0)  # Gets index of the maximum value\n",
        "z = torch.argmin(x, dim=0)  # Gets index of the minimum value\n",
        "mean_x = torch.mean(x.float(), dim=0)  # mean requires x to be float\n",
        "z = torch.eq(x, y)  # Element wise comparison, in this case z = [False, False, False]\n",
        "sorted_y, indices = torch.sort(y, dim=0, descending=False)\n",
        "\n",
        "z = torch.clamp(x, min=0)\n",
        "# All values < 0 set to 0 and values > 0 unchanged (this is exactly ReLU function)\n",
        "# If you want to values over max_val to be clamped, do torch.clamp(x, min=min_val, max=max_val)\n",
        "\n",
        "x = torch.tensor([1, 0, 1, 1, 1], dtype=torch.bool)  # True/False values\n",
        "z = torch.any(x)  # will return True, can also do x.any() instead of torch.any(x)\n",
        "z = torch.all(\n",
        "    x\n",
        ")  # will return False (since not all are True), can also do x.all() instead of torch.all()\n",
        "\n",
        "# ============================================================= #\n",
        "#                        Tensor Indexing                        #\n",
        "# ============================================================= #\n",
        "\n",
        "batch_size = 10\n",
        "features = 25\n",
        "x = torch.rand((batch_size, features))\n",
        "\n",
        "# Get first examples features\n",
        "print(x[0].shape)  # shape [25], this is same as doing x[0,:]\n",
        "\n",
        "# Get the first feature for all examples\n",
        "print(x[:, 0].shape)  # shape [10]\n",
        "\n",
        "# For example: Want to access third example in the batch and the first ten features\n",
        "print(x[2, 0:10].shape)  # shape: [10]\n",
        "\n",
        "# For example we can use this to, assign certain elements\n",
        "x[0, 0] = 100\n",
        "\n",
        "# Fancy Indexing\n",
        "x = torch.arange(10)\n",
        "indices = [2, 5, 8]\n",
        "print(x[indices])  # x[indices] = [2, 5, 8]\n",
        "\n",
        "x = torch.rand((3, 5))\n",
        "rows = torch.tensor([1, 0])\n",
        "cols = torch.tensor([4, 0])\n",
        "print(x[rows, cols])  # Gets second row fifth column and first row first column\n",
        "\n",
        "# More advanced indexing\n",
        "x = torch.arange(10)\n",
        "print(x[(x < 2) | (x > 8)])  # will be [0, 1, 9]\n",
        "print(x[x.remainder(2) == 0])  # will be [0, 2, 4, 6, 8]\n",
        "\n",
        "# Useful operations for indexing\n",
        "print(\n",
        "    torch.where(x > 5, x, x * 2)\n",
        ")  # gives [0, 2, 4, 6, 8, 10, 6, 7, 8, 9], all values x > 5 yield x, else x*2\n",
        "x = torch.tensor([0, 0, 1, 2, 2, 3, 4]).unique()  # x = [0, 1, 2, 3, 4]\n",
        "print(\n",
        "    x.ndimension()\n",
        ")  # The number of dimensions, in this case 1. if x.shape is 5x5x5 ndim would be 3\n",
        "x = torch.arange(10)\n",
        "print(\n",
        "    x.numel()\n",
        ")  # The number of elements in x (in this case it's trivial because it's just a vector)\n",
        "\n",
        "# ============================================================= #\n",
        "#                        Tensor Reshaping                       #\n",
        "# ============================================================= #\n",
        "\n",
        "x = torch.arange(9)\n",
        "\n",
        "# Let's say we want to reshape it to be 3x3\n",
        "x_3x3 = x.view(3, 3)\n",
        "\n",
        "# We can also do (view and reshape are very similar)\n",
        "# and the differences are in simple terms (I'm no expert at this),\n",
        "# is that view acts on contiguous tensors meaning if the\n",
        "# tensor is stored contiguously in memory or not, whereas\n",
        "# for reshape it doesn't matter because it will copy the\n",
        "# tensor to make it contiguously stored, which might come\n",
        "# with some performance loss.\n",
        "x_3x3 = x.reshape(3, 3)\n",
        "\n",
        "# If we for example do:\n",
        "y = x_3x3.t()\n",
        "print(\n",
        "    y.is_contiguous()\n",
        ")  # This will return False and if we try to use view now, it won't work!\n",
        "# y.view(9) would cause an error, reshape however won't\n",
        "\n",
        "# This is because in memory it was stored [0, 1, 2, ... 8], whereas now it's [0, 3, 6, 1, 4, 7, 2, 5, 8]\n",
        "# The jump is no longer 1 in memory for one element jump (matrices are stored as a contiguous block, and\n",
        "# using pointers to construct these matrices). This is a bit complicated and I need to explore this more\n",
        "# as well, at least you know it's a problem to be cautious of! A solution is to do the following\n",
        "print(y.contiguous().view(9))  # Calling .contiguous() before view and it works\n",
        "\n",
        "# Moving on to another operation, let's say we want to add two tensors dimensions togethor\n",
        "x1 = torch.rand(2, 5)\n",
        "x2 = torch.rand(2, 5)\n",
        "print(torch.cat((x1, x2), dim=0).shape)  # Shape: 4x5\n",
        "print(torch.cat((x1, x2), dim=1).shape)  # Shape 2x10\n",
        "\n",
        "# Let's say we want to unroll x1 into one long vector with 10 elements, we can do:\n",
        "z = x1.view(-1)  # And -1 will unroll everything\n",
        "\n",
        "# If we instead have an additional dimension and we wish to keep those as is we can do:\n",
        "batch = 64\n",
        "x = torch.rand((batch, 2, 5))\n",
        "z = x.view(\n",
        "    batch, -1\n",
        ")  # And z.shape would be 64x10, this is very useful stuff and is used all the time\n",
        "\n",
        "# Let's say we want to switch x axis so that instead of 64x2x5 we have 64x5x2\n",
        "# I.e we want dimension 0 to stay, dimension 1 to become dimension 2, dimension 2 to become dimension 1\n",
        "# Basically you tell permute where you want the new dimensions to be, torch.transpose is a special case\n",
        "# of permute (why?)\n",
        "z = x.permute(0, 2, 1)\n",
        "\n",
        "# Splits x last dimension into chunks of 2 (since 5 is not integer div by 2) the last dimension\n",
        "# will be smaller, so it will split it into two tensors: 64x2x3 and 64x2x2\n",
        "z = torch.chunk(x, chunks=2, dim=1)\n",
        "print(z[0].shape)\n",
        "print(z[1].shape)\n",
        "\n",
        "# Let's say we want to add an additional dimension\n",
        "x = torch.arange(\n",
        "    10\n",
        ")  # Shape is [10], let's say we want to add an additional so we have 1x10\n",
        "print(x.unsqueeze(0).shape)  # 1x10\n",
        "print(x.unsqueeze(1).shape)  # 10x1\n",
        "\n",
        "# Let's say we have x which is 1x1x10 and we want to remove a dim so we have 1x10\n",
        "x = torch.arange(10).unsqueeze(0).unsqueeze(1)\n",
        "\n",
        "# Perhaps unsurprisingly\n",
        "z = x.squeeze(1)  # can also do .squeeze(0) both returns 1x10\n",
        "\n",
        "# That was some essential Tensor operations, hopefully you found it useful!"
      ]
    }
  ]
}